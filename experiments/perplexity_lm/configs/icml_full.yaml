# Full ICML Benchmark Configuration for Perplexity (TODO 1.3)
# Run: python -m experiments.perplexity_lm.driver --config experiments/perplexity_lm/configs/icml_full.yaml
#
# This runs comprehensive perplexity evaluation across:
# - WikiText-103 (standard benchmark)
# - C4 (diverse web text)  
# - PG-19 (long book sequences)
#
# With analysis for:
# - Perplexity vs context length scaling
# - Perplexity vs sparsity trade-off curves

global_seed: 42
global_output_dir: results/perplexity/icml

experiments:
  # Experiment 1: WikiText-103 (Standard Benchmark)
  - name: wikitext103_perplexity
    description: Standard perplexity benchmark on WikiText-103
    
    model:
      name: meta-llama/Llama-2-7b-hf
      max_length: 4096
      torch_dtype: float16
      use_flash_attention: true
    
    datasets:
      - wikitext-103
    
    methods:
      - dense
      - h2o
      - cab_v4
      - cab_v3
      - streaming_llm
      - local_strided
      - random
    
    context_length_sweep:
      enabled: true
      context_lengths: [512, 1024, 2048, 4096]
      fixed_sparsity: 0.9
    
    sparsity_sweep:
      enabled: true
      sparsity_levels: [0.0, 0.5, 0.7, 0.8, 0.9, 0.95, 0.99]
      fixed_context_length: 4096
    
    output_dir: results/perplexity/icml/wikitext103
  
  # Experiment 2: C4 (Diverse Web Text)
  - name: c4_perplexity
    description: Perplexity on diverse web text (C4)
    
    model:
      name: meta-llama/Llama-2-7b-hf
      max_length: 4096
      torch_dtype: float16
      use_flash_attention: true
    
    datasets:
      - c4
    
    methods:
      - dense
      - h2o
      - cab_v4
      - streaming_llm
    
    context_length_sweep:
      enabled: true
      context_lengths: [512, 1024, 2048, 4096]
      fixed_sparsity: 0.9
    
    sparsity_sweep:
      enabled: true
      sparsity_levels: [0.0, 0.8, 0.9, 0.95]
      fixed_context_length: 4096
    
    output_dir: results/perplexity/icml/c4
  
  # Experiment 3: PG-19 (Long Books)
  - name: pg19_perplexity
    description: Long-context perplexity on books (PG-19)
    
    model:
      name: meta-llama/Llama-2-7b-hf
      max_length: 16384
      torch_dtype: float16
      use_flash_attention: true
    
    datasets:
      - pg19
    
    methods:
      - dense
      - h2o
      - cab_v4
      - streaming_llm
    
    context_length_sweep:
      enabled: true
      context_lengths: [1024, 2048, 4096, 8192, 16384]
      fixed_sparsity: 0.9
    
    sparsity_sweep:
      enabled: true
      sparsity_levels: [0.0, 0.9, 0.95, 0.99]
      fixed_context_length: 8192
    
    output_dir: results/perplexity/icml/pg19

