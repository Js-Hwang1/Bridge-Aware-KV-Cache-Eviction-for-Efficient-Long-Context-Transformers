# ICML 2025 Full Benchmark Configuration
# ========================================
# This configuration runs the complete benchmark suite for the paper.
# Estimated time: 10-20 hours on 8x A100 GPUs

name: icml_full_benchmark
description: Complete CAB-Attention benchmark for ICML 2025 submission

# Datasets to evaluate
datasets:
  # LongBench QA
  - narrativeqa
  - qasper
  - multifieldqa_en
  - hotpotqa
  - 2wikimqa
  - musique
  
  # LongBench Summarization
  - gov_report
  - qmsum
  - multi_news
  
  # SCROLLS
  - quality
  - qasper_scrolls
  - narrativeqa_scrolls
  - summ_screen_fd
  
  # InfiniteBench (128K+ context)
  - passkey
  - number_string
  - kv_retrieval

# Methods to compare
methods:
  - dense         # Oracle upper bound
  - h2o           # Primary baseline
  - cab_v4        # Our method (hybrid)
  - cab_v3        # Pure FRC ablation
  - streaming_llm # Streaming baseline
  - local_strided # Pattern baseline
  - random        # Lower bound

# Sparsity levels for Pareto analysis
sparsity_levels:
  - 0.5
  - 0.7
  - 0.8
  - 0.9
  - 0.95
  - 0.99

# Model configuration
model:
  name: meta-llama/Llama-2-7b-hf
  torch_dtype: float16
  max_length: 8192
  max_new_tokens: 256
  use_flash_attention: true

# Evaluation settings
max_samples: null  # Use all samples
batch_size: 1
seed: 42

# Output
output_dir: results/icml_full
save_predictions: true
save_attention_patterns: false  # Too memory-intensive for full run

# Logging
wandb_project: cab-attention-icml2025

