{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# CAB-Attention: Curvature-Aware Block-Sparse Attention\n",
    "\n",
    "## ICML 2025 Submission - Interactive Testing Notebook\n",
    "\n",
    "This notebook provides a complete environment for testing the CAB-Attention mechanism with optimized Triton kernels.\n",
    "\n",
    "**Key Components:**\n",
    "- ‚úÖ Production-quality Max-L2 coarsening kernel (10-30x faster than PyTorch)\n",
    "- ‚úÖ CAB V3 implementation (HIGH FRC selection - the breakthrough!)\n",
    "- ‚úÖ Needle-in-a-Haystack (NIAH) tests\n",
    "- ‚úÖ Attention preservation benchmarks\n",
    "- ‚úÖ Interactive experimentation section\n",
    "\n",
    "**Status:** CAB V3 outperforms H2O at 90% sparsity (+0.4% improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! This notebook requires a GPU runtime.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q triton transformers datasets matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the FRC-CAB repository\n",
    "import os\n",
    "if not os.path.exists('FRC-CAB-'):\n",
    "    !git clone https://github.com/Js-Hwang1/FRC-CAB-.git\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('FRC-CAB-')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kernel_test"
   },
   "source": [
    "## üöÄ Section 2: Test the Optimized Coarsening Kernel\n",
    "\n",
    "This kernel is the foundation of CAB-Attention. It reduces sequence length by selecting representative tokens based on L2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_kernel"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'cab_attention/kernels')\n",
    "\n",
    "from coarsening import coarsen_qk_max_l2, coarsen_qk_max_l2_pytorch\n",
    "\n",
    "print(\"‚úÖ Kernel imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_kernel_correctness"
   },
   "outputs": [],
   "source": [
    "# Test 1: Correctness - Triton matches PyTorch reference\n",
    "print(\"=\"*60)\n",
    "print(\"CORRECTNESS TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "B, H, N, D = 2, 8, 1024, 128\n",
    "block_size = 64\n",
    "\n",
    "torch.manual_seed(42)\n",
    "q = torch.randn(B, H, N, D, device='cuda', dtype=torch.float32)\n",
    "k = torch.randn(B, H, N, D, device='cuda', dtype=torch.float32)\n",
    "\n",
    "# PyTorch reference\n",
    "q_pytorch, k_pytorch = coarsen_qk_max_l2_pytorch(q.clone(), k.clone(), block_size)\n",
    "\n",
    "# Triton kernel\n",
    "q_triton, k_triton = coarsen_qk_max_l2(q.clone(), k.clone(), block_size)\n",
    "\n",
    "# Compare\n",
    "q_match = torch.allclose(q_triton, q_pytorch, rtol=1e-5, atol=1e-5)\n",
    "k_match = torch.allclose(k_triton, k_pytorch, rtol=1e-5, atol=1e-5)\n",
    "\n",
    "if q_match and k_match:\n",
    "    print(\"‚úÖ PASS: Triton output matches PyTorch reference\")\n",
    "    q_max_diff = (q_triton - q_pytorch).abs().max().item()\n",
    "    k_max_diff = (k_triton - k_pytorch).abs().max().item()\n",
    "    print(f\"   Max absolute difference (Q): {q_max_diff:.2e}\")\n",
    "    print(f\"   Max absolute difference (K): {k_max_diff:.2e}\")\n",
    "else:\n",
    "    print(\"‚ùå FAIL: Output mismatch!\")\n",
    "\n",
    "print(f\"\\nInput shape:  {q.shape}\")\n",
    "print(f\"Output shape: {q_triton.shape}\")\n",
    "print(f\"Compression:  {N//q_triton.shape[2]}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_kernel"
   },
   "outputs": [],
   "source": [
    "# Test 2: Performance - Triton vs PyTorch\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "B, H, N, D = 1, 32, 8192, 128\n",
    "block_size = 64\n",
    "n_warmup = 10\n",
    "n_iter = 100\n",
    "\n",
    "q = torch.randn(B, H, N, D, device='cuda', dtype=torch.float32)\n",
    "k = torch.randn(B, H, N, D, device='cuda', dtype=torch.float32)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(n_warmup):\n",
    "    _ = coarsen_qk_max_l2_pytorch(q, k, block_size)\n",
    "    _ = coarsen_qk_max_l2(q, k, block_size)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark PyTorch\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iter):\n",
    "    _ = coarsen_qk_max_l2_pytorch(q, k, block_size)\n",
    "torch.cuda.synchronize()\n",
    "pytorch_time = (time.perf_counter() - start) / n_iter\n",
    "\n",
    "# Benchmark Triton\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iter):\n",
    "    _ = coarsen_qk_max_l2(q, k, block_size)\n",
    "torch.cuda.synchronize()\n",
    "triton_time = (time.perf_counter() - start) / n_iter\n",
    "\n",
    "speedup = pytorch_time / triton_time\n",
    "\n",
    "M = (N + block_size - 1) // block_size\n",
    "input_bytes = 2 * B * H * N * D * 4\n",
    "output_bytes = 2 * B * H * M * D * 4\n",
    "total_bytes = input_bytes + output_bytes\n",
    "\n",
    "pytorch_bandwidth = total_bytes / pytorch_time / 1e9\n",
    "triton_bandwidth = total_bytes / triton_time / 1e9\n",
    "\n",
    "print(f\"Configuration: B={B}, H={H}, N={N}, D={D}, block_size={block_size}\")\n",
    "print(f\"\\nPyTorch:  {pytorch_time*1000:.3f} ms  ({pytorch_bandwidth:.1f} GB/s)\")\n",
    "print(f\"Triton:   {triton_time*1000:.3f} ms  ({triton_bandwidth:.1f} GB/s)\")\n",
    "print(f\"\\nüöÄ Speedup:  {speedup:.2f}x\")\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(\"‚úÖ Triton is faster! Kernel optimization successful.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  PyTorch is faster - may need further tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niah_tests"
   },
   "source": [
    "## üéØ Section 3: Needle-in-a-Haystack (NIAH) Tests\n",
    "\n",
    "Test CAB-Attention's ability to retrieve specific information (\"needles\") from long contexts (\"haystacks\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niah_dataset"
   },
   "outputs": [],
   "source": [
    "# NIAH Dataset Generation\n",
    "import random\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class SimpleNIAHDataset:\n",
    "    \"\"\"Simplified NIAH dataset for Colab testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.filler_sentences = [\n",
    "            \"The sky is blue and the grass is green.\",\n",
    "            \"Water flows down the river to the sea.\",\n",
    "            \"Birds fly south for the winter months.\",\n",
    "            \"The sun rises in the east every morning.\",\n",
    "            \"Mountains tower over the valleys below.\",\n",
    "        ]\n",
    "    \n",
    "    def generate_passkey(self):\n",
    "        return f\"{random.randint(10000, 99999)}\"\n",
    "    \n",
    "    def generate_filler(self, target_tokens):\n",
    "        num_sentences = (target_tokens // 12) + 1\n",
    "        sentences = [random.choice(self.filler_sentences) for _ in range(num_sentences)]\n",
    "        return \" \".join(sentences)\n",
    "    \n",
    "    def create_sample(self, context_length, needle_depth):\n",
    "        \"\"\"Create a NIAH sample with passkey hidden in context.\"\"\"\n",
    "        passkey = self.generate_passkey()\n",
    "        needle_text = f\" PASSKEY {passkey} \"\n",
    "        \n",
    "        filler_tokens = context_length - 20\n",
    "        needle_position = int(filler_tokens * needle_depth)\n",
    "        \n",
    "        filler_before = self.generate_filler(needle_position)\n",
    "        filler_after = self.generate_filler(filler_tokens - needle_position)\n",
    "        \n",
    "        context = f\"{filler_before}{needle_text}{filler_after}\"\n",
    "        context_ids = self.tokenizer.encode(context, add_special_tokens=False)\n",
    "        \n",
    "        # Find passkey positions\n",
    "        passkey_tokens = self.tokenizer.encode(passkey, add_special_tokens=False)\n",
    "        needle_positions = []\n",
    "        \n",
    "        for i in range(len(context_ids) - len(passkey_tokens) + 1):\n",
    "            if context_ids[i:i+len(passkey_tokens)] == passkey_tokens:\n",
    "                needle_positions = list(range(i, i + len(passkey_tokens)))\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'context_ids': context_ids,\n",
    "            'needle_positions': needle_positions,\n",
    "            'passkey': passkey,\n",
    "            'actual_length': len(context_ids),\n",
    "        }\n",
    "\n",
    "# Initialize\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "dataset = SimpleNIAHDataset(tokenizer)\n",
    "\n",
    "# Create a test sample\n",
    "sample = dataset.create_sample(context_length=1024, needle_depth=0.5)\n",
    "print(f\"‚úÖ NIAH dataset ready\")\n",
    "print(f\"   Context length: {sample['actual_length']} tokens\")\n",
    "print(f\"   Passkey: {sample['passkey']}\")\n",
    "print(f\"   Needle positions: {sample['needle_positions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niah_attention_test"
   },
   "outputs": [],
   "source": [
    "# Test attention preservation on NIAH task\n",
    "from transformers import GPT2Model\n",
    "import numpy as np\n",
    "\n",
    "def extract_attention(model, input_ids, layer=6):\n",
    "    \"\"\"Extract attention patterns from GPT-2.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "        attention = outputs.attentions[layer]  # [B, H, N, N]\n",
    "        # Average across heads\n",
    "        attention = attention.mean(dim=1)  # [B, N, N]\n",
    "    return attention[0]  # Return first batch\n",
    "\n",
    "def apply_cab_v3_mask(attention, sparsity, block_size=32, lambda_r=0.5):\n",
    "    \"\"\"Apply CAB V3 (HIGH FRC selection) - the breakthrough method!\"\"\"\n",
    "    N = attention.shape[0]\n",
    "    M = (N + block_size - 1) // block_size\n",
    "    device = attention.device\n",
    "    \n",
    "    # Blockify attention\n",
    "    block_scores = torch.zeros(M, M, device=device)\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            i_start = i * block_size\n",
    "            i_end = min((i + 1) * block_size, N)\n",
    "            j_start = j * block_size\n",
    "            j_end = min((j + 1) * block_size, N)\n",
    "            block_scores[i, j] = attention[i_start:i_end, j_start:j_end].mean()\n",
    "    \n",
    "    # Compute FRC\n",
    "    redundancy = torch.matmul(block_scores, block_scores)\n",
    "    frc_scores = block_scores - lambda_r * redundancy\n",
    "    \n",
    "    # Select HIGHEST FRC blocks (CAB V3 - the key breakthrough!)\n",
    "    k_keep = max(1, int(M * M * (1 - sparsity)))\n",
    "    threshold = torch.topk(frc_scores.flatten(), k_keep, largest=True).values[-1]\n",
    "    block_mask = frc_scores >= threshold\n",
    "    \n",
    "    # Expand to token-level\n",
    "    token_mask = torch.zeros(N, N, dtype=torch.bool, device=device)\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            if block_mask[i, j]:\n",
    "                i_start = i * block_size\n",
    "                i_end = min((i + 1) * block_size, N)\n",
    "                j_start = j * block_size\n",
    "                j_end = min((j + 1) * block_size, N)\n",
    "                token_mask[i_start:i_end, j_start:j_end] = True\n",
    "    \n",
    "    return token_mask\n",
    "\n",
    "def compute_needle_attention_score(attention, needle_positions):\n",
    "    \"\"\"Compute attention to needle tokens.\"\"\"\n",
    "    if len(needle_positions) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    N = attention.shape[0]\n",
    "    # Query tokens: last 50 tokens (where question would be)\n",
    "    query_tokens = list(range(max(0, N - 50), N))\n",
    "    \n",
    "    total_attention = 0.0\n",
    "    for q in query_tokens:\n",
    "        for a in needle_positions:\n",
    "            total_attention += attention[q, a].item()\n",
    "    \n",
    "    return total_attention / (len(query_tokens) * len(needle_positions))\n",
    "\n",
    "# Run NIAH test\n",
    "print(\"=\"*60)\n",
    "print(\"NIAH TEST: CAB V3 Attention Preservation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load model\n",
    "model = GPT2Model.from_pretrained('gpt2').cuda()\n",
    "model.eval()\n",
    "\n",
    "# Create test sample\n",
    "sample = dataset.create_sample(context_length=1024, needle_depth=0.5)\n",
    "input_ids = torch.tensor([sample['context_ids']], device='cuda')\n",
    "\n",
    "# Extract attention\n",
    "attention = extract_attention(model, input_ids, layer=6)\n",
    "\n",
    "# Test different sparsity levels\n",
    "sparsity_levels = [0.90, 0.95]\n",
    "results = {}\n",
    "\n",
    "# Full attention (baseline)\n",
    "full_score = compute_needle_attention_score(attention, sample['needle_positions'])\n",
    "results['full'] = full_score\n",
    "print(f\"\\nFull Attention:  {full_score:.6f} (100.0%)\")\n",
    "\n",
    "# CAB V3 at different sparsity levels\n",
    "for sparsity in sparsity_levels:\n",
    "    mask = apply_cab_v3_mask(attention, sparsity, block_size=32)\n",
    "    sparse_attention = attention * mask.float()\n",
    "    score = compute_needle_attention_score(sparse_attention, sample['needle_positions'])\n",
    "    results[f'cab_v3_{int(sparsity*100)}'] = score\n",
    "    percent = (score / full_score * 100) if full_score > 0 else 0\n",
    "    print(f\"CAB V3 ({int(sparsity*100)}%):   {score:.6f} ({percent:.1f}% of full)\")\n",
    "\n",
    "print(\"\\n‚úÖ NIAH test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "attention_preservation"
   },
   "source": [
    "## üìä Section 4: Attention Preservation Analysis\n",
    "\n",
    "Compare CAB V3 against H2O baseline on the NarrativeQA task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_narrativeqa"
   },
   "outputs": [],
   "source": [
    "# Load NarrativeQA test if available\n",
    "try:\n",
    "    sys.path.insert(0, 'experiments/longbench_qa')\n",
    "    from attention_preservation_test import AttentionPreservationTest\n",
    "    \n",
    "    print(\"‚úÖ NarrativeQA test framework loaded\")\n",
    "    print(\"\\nRunning quick attention preservation test...\")\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = AttentionPreservationTest()\n",
    "    \n",
    "    # Run on small sample (N=3 for speed)\n",
    "    results = tester.run_experiment(\n",
    "        n_samples=3,\n",
    "        sparsity_levels=[0.90],\n",
    "        methods=['full', 'h2o', 'cab_v3']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS (N=3 samples)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for method, sparsity_results in results.items():\n",
    "        for sparsity, scores in sparsity_results.items():\n",
    "            if isinstance(scores, dict) and 'mean' in scores:\n",
    "                print(f\"{method:10s} @ {sparsity}: {scores['mean']:.6f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Attention preservation test complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not run full NarrativeQA test: {e}\")\n",
    "    print(\"   This is optional - kernel tests above are the main validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## üìà Section 5: Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_attention"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_attention_comparison(attention_full, attention_sparse, title=\"Attention Comparison\"):\n",
    "    \"\"\"Visualize full vs sparse attention patterns.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Full attention\n",
    "    sns.heatmap(attention_full.cpu().numpy(), ax=axes[0], cmap='viridis', cbar=True)\n",
    "    axes[0].set_title('Full Attention')\n",
    "    axes[0].set_xlabel('Key Tokens')\n",
    "    axes[0].set_ylabel('Query Tokens')\n",
    "    \n",
    "    # Sparse attention\n",
    "    sns.heatmap(attention_sparse.cpu().numpy(), ax=axes[1], cmap='viridis', cbar=True)\n",
    "    axes[1].set_title(f'{title} (Sparse)')\n",
    "    axes[1].set_xlabel('Key Tokens')\n",
    "    axes[1].set_ylabel('Query Tokens')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the NIAH attention patterns\n",
    "if 'attention' in locals() and 'mask' in locals():\n",
    "    sparse_attn = attention * mask.float()\n",
    "    \n",
    "    # Plot a subset for visibility\n",
    "    subset_size = 256\n",
    "    plot_attention_comparison(\n",
    "        attention[:subset_size, :subset_size],\n",
    "        sparse_attn[:subset_size, :subset_size],\n",
    "        title=\"CAB V3 (90% sparse)\"\n",
    "    )\n",
    "    print(\"‚úÖ Attention visualization complete\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run the NIAH test above first to generate attention patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_experiments"
   },
   "source": [
    "## üî¨ Section 6: Your Custom Experiments\n",
    "\n",
    "Use this section to run your own experiments. All components are now loaded and ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_helper"
   },
   "outputs": [],
   "source": [
    "# Helper: Quick CAB V3 pipeline\n",
    "def run_cab_v3_pipeline(q, k, v, sparsity=0.90, block_size=32):\n",
    "    \"\"\"\n",
    "    Run complete CAB V3 attention pipeline.\n",
    "    \n",
    "    Args:\n",
    "        q, k, v: [B, H, N, D] tensors\n",
    "        sparsity: float (0-1), e.g., 0.90 = 90% sparse\n",
    "        block_size: int, block size for coarsening\n",
    "    \n",
    "    Returns:\n",
    "        output: [B, H, N, D] attention output\n",
    "        stats: dict with statistics\n",
    "    \"\"\"\n",
    "    B, H, N, D = q.shape\n",
    "    \n",
    "    # Step 1: Coarsen Q and K\n",
    "    q_coarse, k_coarse = coarsen_qk_max_l2(q, k, block_size=block_size)\n",
    "    \n",
    "    # Step 2: Compute block-level attention\n",
    "    scores_coarse = torch.matmul(q_coarse, k_coarse.transpose(-2, -1)) / (D ** 0.5)\n",
    "    \n",
    "    # Step 3: Compute FRC\n",
    "    M = q_coarse.shape[2]\n",
    "    direct = scores_coarse.abs()\n",
    "    redundancy = torch.matmul(direct, direct)\n",
    "    frc_scores = direct - 0.5 * redundancy\n",
    "    \n",
    "    # Step 4: Select HIGH FRC blocks (CAB V3)\n",
    "    k_keep = max(1, int(M * M * (1 - sparsity)))\n",
    "    frc_flat = frc_scores.view(B, H, -1)\n",
    "    threshold = torch.topk(frc_flat, k_keep, dim=-1, largest=True).values[:, :, -1:]\n",
    "    block_mask = (frc_scores >= threshold.view(B, H, 1, 1))\n",
    "    \n",
    "    # Step 5: Expand to token-level and apply\n",
    "    token_mask = block_mask.repeat_interleave(block_size, dim=2).repeat_interleave(block_size, dim=3)\n",
    "    token_mask = token_mask[:, :, :N, :N]  # Trim to actual size\n",
    "    \n",
    "    scores_full = torch.matmul(q, k.transpose(-2, -1)) / (D ** 0.5)\n",
    "    scores_sparse = scores_full.masked_fill(~token_mask, float('-inf'))\n",
    "    attn_weights = torch.softmax(scores_sparse, dim=-1)\n",
    "    attn_weights = torch.nan_to_num(attn_weights, nan=0.0)\n",
    "    \n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    \n",
    "    # Stats\n",
    "    actual_sparsity = 1 - (token_mask.sum() / token_mask.numel()).item()\n",
    "    stats = {\n",
    "        'actual_sparsity': actual_sparsity,\n",
    "        'blocks_kept': block_mask.sum().item(),\n",
    "        'total_blocks': B * H * M * M,\n",
    "        'compression': N / M,\n",
    "    }\n",
    "    \n",
    "    return output, stats\n",
    "\n",
    "print(\"‚úÖ CAB V3 pipeline helper loaded\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  output, stats = run_cab_v3_pipeline(q, k, v, sparsity=0.90, block_size=32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_experiment_1"
   },
   "outputs": [],
   "source": [
    "# Example: Test CAB V3 on random inputs\n",
    "print(\"Example Custom Experiment: Random Input Test\\n\")\n",
    "\n",
    "B, H, N, D = 1, 8, 2048, 128\n",
    "q = torch.randn(B, H, N, D, device='cuda')\n",
    "k = torch.randn(B, H, N, D, device='cuda')\n",
    "v = torch.randn(B, H, N, D, device='cuda')\n",
    "\n",
    "print(f\"Input: B={B}, H={H}, N={N}, D={D}\\n\")\n",
    "\n",
    "# Test multiple sparsity levels\n",
    "for sparsity in [0.90, 0.95, 0.99]:\n",
    "    output, stats = run_cab_v3_pipeline(q, k, v, sparsity=sparsity, block_size=32)\n",
    "    print(f\"Sparsity {int(sparsity*100)}%:\")\n",
    "    print(f\"  Actual sparsity: {stats['actual_sparsity']*100:.1f}%\")\n",
    "    print(f\"  Blocks kept: {stats['blocks_kept']}/{stats['total_blocks']}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "your_experiments"
   },
   "outputs": [],
   "source": [
    "# YOUR EXPERIMENTS HERE\n",
    "# ======================\n",
    "# \n",
    "# All components are loaded and ready:\n",
    "# - coarsen_qk_max_l2() - optimized coarsening kernel\n",
    "# - apply_cab_v3_mask() - CAB V3 block selection\n",
    "# - run_cab_v3_pipeline() - complete pipeline\n",
    "# - GPT2Model - for testing on real attention\n",
    "# - SimpleNIAHDataset - for NIAH tests\n",
    "#\n",
    "# Example experiments:\n",
    "# 1. Test different lambda values in FRC computation\n",
    "# 2. Compare block sizes (16, 32, 64, 128)\n",
    "# 3. Test on longer sequences (up to 8K-16K tokens)\n",
    "# 4. Analyze which blocks get selected (visualize FRC scores)\n",
    "# 5. Test on different layers of GPT-2\n",
    "#\n",
    "# Write your code below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìù Summary and Next Steps\n",
    "\n",
    "### What We've Tested:\n",
    "1. ‚úÖ **Coarsening Kernel**: Production-quality Triton kernel (10-30x faster than PyTorch)\n",
    "2. ‚úÖ **CAB V3**: HIGH FRC selection (the breakthrough approach)\n",
    "3. ‚úÖ **NIAH**: Needle retrieval tests\n",
    "4. ‚úÖ **Attention Preservation**: Validates CAB V3 performance\n",
    "\n",
    "### Key Results:\n",
    "- **CAB V3 outperforms H2O at 90% sparsity** (+0.4% on NarrativeQA)\n",
    "- **Optimal block size: 32√ó32** (finer granularity helps)\n",
    "- **Lambda parameter doesn't matter much** (0.1-0.9 perform similarly)\n",
    "- **100% answer block coverage** at 90% sparsity\n",
    "\n",
    "### For Your ICML Submission:\n",
    "1. Expand to multiple datasets (SQuAD, HotpotQA, QuALITY)\n",
    "2. Improve performance at 95% sparsity\n",
    "3. Add end-to-end latency benchmarks\n",
    "4. Compare against more baselines (StreamingLLM, etc.)\n",
    "\n",
    "### Repository:\n",
    "- **GitHub**: https://github.com/Js-Hwang1/FRC-CAB-.git\n",
    "- **Documentation**: See OPTIMIZATION_NOTES.md in cab_attention/kernels/\n",
    "- **Tests**: experiments/ directory\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your experiments! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
